{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shweta_Malla_Lab1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shwetamalla14/FirstNeuralNetwork/blob/main/Shweta_Malla_Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JquiOxyNy1HS",
        "outputId": "ae116da1-dacf-4bec-ba35-16f51fe3a2ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# load text data and convert the label/sentiment into corresponding numeric values: '\n",
        "# possible packages you might need are: pandas, numpy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# read the training data\n",
        "df_train=pd.read_csv('facebook_comments.csv',header=None,names=['text','sentiment'], encoding='iso-8859-1',lineterminator='\\n')\n",
        "#we now create a new column in the DF\n",
        "sent= {'positive':2, 'neutral':1, 'negative':0}\n",
        "#map every element in the series in the sentiment column:\n",
        "df_train['labels']=df_train['sentiment'].str.strip().map(sent) \n",
        "\n",
        "# get texts and labels\n",
        "training_texts = df_train.text.values\n",
        "labels = df_train.labels.values\n",
        "\n",
        "#checking data type of texts and labels(they are now numpy arrays)\n",
        "print(type(training_texts),type(labels))\n",
        "df_train.head()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Heres a single  to add  to Kindle. Just read t...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>If you tire of Non-Fiction.. Check out http://...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ghost of Round Island is supposedly nonfiction.</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why is Barnes and Nobles version of the Kindle...</td>\n",
              "      <td>negative</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@Maria:  Do you mean the Nook?  Be careful  bo...</td>\n",
              "      <td>positive</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sentiment  labels\n",
              "0  Heres a single  to add  to Kindle. Just read t...    neutral       1\n",
              "1  If you tire of Non-Fiction.. Check out http://...    neutral       1\n",
              "2   Ghost of Round Island is supposedly nonfiction.     neutral       1\n",
              "3  Why is Barnes and Nobles version of the Kindle...   negative       0\n",
              "4  @Maria:  Do you mean the Nook?  Be careful  bo...   positive       2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6Yjlzxw0Ux5"
      },
      "source": [
        "## **PREPROCESS DATA **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmPC7nz-0cM3",
        "outputId": "c855ee4e-c979-4052-bdf6-46d801b6aeb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# preprocess the loaded textual data, including removing stopwords, stemming, and tok\n",
        "# represent each document (i.e., comment) using TF-IDF strategy. The features are the\n",
        "# possible packages you might need are: scikit-learn, numpy\n",
        "\n",
        "#For the features, we are using uni-grams (We can use bi-grams and tri-grams too)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# tokenize and create a document-feature matrix X and a label vector Y\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=500, ngram_range=(1,1))\n",
        "instances = vectorizer.fit_transform(training_texts)\n",
        "X= instances.toarray()\n",
        "Y= labels\n",
        "\n",
        "# print out the shape of X and Y\n",
        "print(X.shape,',',Y.shape)\n",
        "\n",
        "#first 10 records of Y\n",
        "print(Y[:10])\n",
        "print(X[0,:50]) #first 50 columns These are the TF-IDF values"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1999, 500) , (1999,)\n",
            "[1 1 1 0 2 2 2 0 2 0]\n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.28915636 0.         0.         0.\n",
            " 0.         0.         0.2971592  0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbvzFgXJLEvE"
      },
      "source": [
        "### Traditional Machine Learning Models: Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab9l63uSLHuu",
        "outputId": "cb6727c4-00d9-400a-e722-a8fe2a0ea815",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# using 10-fold cross-validation to show the prediction accuracy\n",
        "# possible packages you might need are: scikit-learn, numpy\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#We want to fix random generator so we set random_state to a random number\n",
        "kfold= KFold(n_splits=10, shuffle=True,random_state=2020)\n",
        "#estimator is the number of trees by default we have 100\n",
        "rf_model = RandomForestClassifier(criterion ='entropy', max_depth=2,random_state=2020) \n",
        "rf_cvscores=[]\n",
        "\n",
        "#We train the model, get the accuracy on the validation set\n",
        "for train_idx, val_idx in kfold.split(X):\n",
        "  rf_model.fit(X[train_idx], Y[train_idx])\n",
        "  acc= rf_model.score(X[val_idx],Y[val_idx])\n",
        "  rf_cvscores.append(acc)\n",
        "\n",
        "print(\"Random Forest - mean: %.4f%% (std: +/- %.4f%%)\" % (np.mean(rf_cvscores)*100, np.std(rf_cvscores)*100))\n",
        "#We get the mean accuracy of around 64%, which is not that great"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forest - mean: 64.1332% (std: +/- 2.0919%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNaQd9IjgySL"
      },
      "source": [
        "### Fully connected feedforward Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T54kmtvUg0CK"
      },
      "source": [
        "# Design your own network with the following requirements:\n",
        "# 1. Having dropout\n",
        "# 2. Separate the dataset into training and validation (80-20%)\n",
        "# 3. The prediction accuracy on the validation set should be at least 50% for this"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsRbekGPg_nO"
      },
      "source": [
        "# possible packages you might need are: scikit-learn, numpy, torch\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.optim as optim"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK4mc9swhg0L"
      },
      "source": [
        "# convert your numpy array to TensorDataset and create a data loader for training and\n",
        "# some hyperparameters: input dimension, output dimension, batch size, number of epoc\n",
        "\n",
        "epochs = 50\n",
        "lr = 0.01 #Learning rate  \n",
        "indim = X.shape[1] #input dimensionality, for each instance it will be 500 in our case\n",
        "outdim = 3 #output dimensionality, 3 categories - Neatural, psoitive, negative \n",
        "drate = 0.7\n",
        "batch_size = 40\n",
        "\n",
        "\n",
        "X_tensor = torch.from_numpy(X)\n",
        "Y_tensor = torch.from_numpy(Y)\n",
        "\n",
        "dataset = TensorDataset(X_tensor,Y_tensor)\n",
        "train_size = int(0.8*len(dataset)) #80% for training set data \n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "#We do a random split provided by Torch , length we can have a tuple or array\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, lengths=[train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, shuffle=True,batch_size=batch_size)\n",
        "val_loader = DataLoader(val_dataset, shuffle=True,batch_size=batch_size)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h95rgcVWpmDY"
      },
      "source": [
        "## Build the network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bFzOsv1ExER",
        "outputId": "cab533d1-9c56-40d6-f5d3-7e1705f654ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "class SentimentNetwork(nn.Module):\n",
        "  def __init__(self, indim, outdim, drate):\n",
        "    super(SentimentNetwork,self).__init__()\n",
        "    self.fc1 = nn.Linear(indim,100,bias = True)\n",
        "    self.fc2 = nn.Linear(100, 50,bias = True)\n",
        "    self.fc3 = nn.Linear(50, outdim,bias = True)    #creating 2 hidden layers for our model with dropout rate of 0.7\n",
        "    self.do1 = nn.Dropout(0.7)\n",
        "    self.do2 = nn.Dropout(0.7) \n",
        "    self.bn1 = nn.BatchNorm1d(100)\n",
        "    self.bn2 = nn.BatchNorm1d(50)\n",
        "     \n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.bn1(x)\n",
        "    x = self.do1(x)\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.bn2(x)\n",
        "    x = self.do2(x)\n",
        "    \n",
        "    return F.log_softmax(self.fc3(x))\n",
        "    #return x\n",
        "\n",
        "# create a model\n",
        "model = SentimentNetwork(indim,outdim,drate)\n",
        "#model = SentimentNetwork(input_dim, output_dim, dropout_rate)\n",
        "print(model)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentNetwork(\n",
            "  (fc1): Linear(in_features=500, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
            "  (fc3): Linear(in_features=50, out_features=3, bias=True)\n",
            "  (do1): Dropout(p=0.7, inplace=False)\n",
            "  (do2): Dropout(p=0.7, inplace=False)\n",
            "  (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlS4xAXez7RD"
      },
      "source": [
        "## Create a training function to train the model and an evaluation function to evaluate the performance on the separate validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEOKSTnDz4Qs"
      },
      "source": [
        "#define a training process function\n",
        "\n",
        "def calculate_accuracy(y_pred, y_true):\n",
        "  predicted= np.argmax(y_pred.detach().numpy(), axis=1)\n",
        "  #predicted = torch.argmax(y_pred, dim=1)\n",
        "  \n",
        "  #predicted = torch.max(y_pred, 1)\n",
        "  return np.sum(y_true.detach().numpy() == predicted)/batch_size\n",
        "  #return (y_true == predicted).sum.mean()\n",
        "\n",
        "\n",
        "# creating losss function \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "#creating optimizer \n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "def train(model, train_loader, optimizer, criterion):\n",
        "  epoch_loss, epoch_acc = 0.0,0.0 # the loss and accuracy for each epoch\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for batch_x, batch_y in train_loader:\n",
        "    y_pred = model(batch_x.float())   #predictions \n",
        "    loss = criterion(y_pred, batch_y) #calculating the loss \n",
        "    optimizer.zero_grad()       # Intialize the hidden weight to all zeros\n",
        "    loss.backward()           # Backward pass to compute the weight\n",
        "    optimizer.step()          ## Optimizer to update the weights of hidden nodes\n",
        "    \n",
        "    train_acc=calculate_accuracy(y_pred,batch_y)  #calculating the accuraxy from the function \n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += train_acc\n",
        "\n",
        "    #calculate avg epoch loss and accuracy\n",
        "    tl= len(train_loader)\n",
        "    avg_acc_train = epoch_acc/tl\n",
        "    avg_epoch_train = epoch_acc/tl\n",
        "\n",
        "  return avg_epoch_train, avg_acc_train"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2L5gAcy0T-r"
      },
      "source": [
        "# define a validation/evaluation process function\n",
        "\n",
        "\n",
        "def evaluate(model, val_loader, criterion):\n",
        "  epoch_loss, epoch_acc = 0.0,0.0 # the loss and accuracy for each epoch\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_x, batch_y in val_loader:\n",
        "      y_pred = model(batch_x.float())     #predictions \n",
        "      loss = criterion(y_pred, batch_y)   #calculating the loss \n",
        "      val_acc = calculate_accuracy(y_pred, batch_y)     #calculating the accuraxy from the function\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += val_acc\n",
        "\n",
        "\n",
        "\n",
        "    #calculate avg epoch loss and accuracy \n",
        "    vl= len(val_loader)\n",
        "    avg_acc_val = epoch_acc/vl\n",
        "    avg_epoch_val = epoch_loss/vl\n",
        "      \n",
        "    return avg_epoch_val, avg_acc_val"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbDm0mOW7hZx"
      },
      "source": [
        "## Main starting point: train the model and evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhpafGgu7kYu",
        "outputId": "322aabdb-6ba5-4367-ea59-807b65916c6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# real training and evaluation process\n",
        "for epoch in range(epochs):\n",
        "  train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "  valid_loss, valid_acc = evaluate(model, val_loader, criterion)\n",
        "  print(f'Epoch: {epoch+1:02}')\n",
        "  print(f'\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
        "  print(f'\\t Val. Loss: {valid_loss:.4f} | Val. Acc: {valid_acc:.4f}')\n",
        "\n",
        "  "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 0.5419 | Train Acc: 0.5419\n",
            "\t Val. Loss: 0.8111 | Val. Acc: 0.6750\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.6994 | Train Acc: 0.6994\n",
            "\t Val. Loss: 0.5711 | Val. Acc: 0.7825\n",
            "Epoch: 03\n",
            "\tTrain Loss: 0.7244 | Train Acc: 0.7244\n",
            "\t Val. Loss: 0.5147 | Val. Acc: 0.8050\n",
            "Epoch: 04\n",
            "\tTrain Loss: 0.7525 | Train Acc: 0.7525\n",
            "\t Val. Loss: 0.4862 | Val. Acc: 0.8250\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.7850 | Train Acc: 0.7850\n",
            "\t Val. Loss: 0.4666 | Val. Acc: 0.8225\n",
            "Epoch: 06\n",
            "\tTrain Loss: 0.7981 | Train Acc: 0.7981\n",
            "\t Val. Loss: 0.4633 | Val. Acc: 0.8100\n",
            "Epoch: 07\n",
            "\tTrain Loss: 0.8106 | Train Acc: 0.8106\n",
            "\t Val. Loss: 0.4336 | Val. Acc: 0.8250\n",
            "Epoch: 08\n",
            "\tTrain Loss: 0.8325 | Train Acc: 0.8325\n",
            "\t Val. Loss: 0.4523 | Val. Acc: 0.8275\n",
            "Epoch: 09\n",
            "\tTrain Loss: 0.8306 | Train Acc: 0.8306\n",
            "\t Val. Loss: 0.4162 | Val. Acc: 0.8425\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.8175 | Train Acc: 0.8175\n",
            "\t Val. Loss: 0.4154 | Val. Acc: 0.8450\n",
            "Epoch: 11\n",
            "\tTrain Loss: 0.8375 | Train Acc: 0.8375\n",
            "\t Val. Loss: 0.3966 | Val. Acc: 0.8675\n",
            "Epoch: 12\n",
            "\tTrain Loss: 0.8506 | Train Acc: 0.8506\n",
            "\t Val. Loss: 0.3810 | Val. Acc: 0.8800\n",
            "Epoch: 13\n",
            "\tTrain Loss: 0.8487 | Train Acc: 0.8487\n",
            "\t Val. Loss: 0.3985 | Val. Acc: 0.8675\n",
            "Epoch: 14\n",
            "\tTrain Loss: 0.8513 | Train Acc: 0.8513\n",
            "\t Val. Loss: 0.3829 | Val. Acc: 0.8975\n",
            "Epoch: 15\n",
            "\tTrain Loss: 0.8644 | Train Acc: 0.8644\n",
            "\t Val. Loss: 0.3866 | Val. Acc: 0.8775\n",
            "Epoch: 16\n",
            "\tTrain Loss: 0.8725 | Train Acc: 0.8725\n",
            "\t Val. Loss: 0.3776 | Val. Acc: 0.8850\n",
            "Epoch: 17\n",
            "\tTrain Loss: 0.8738 | Train Acc: 0.8738\n",
            "\t Val. Loss: 0.3783 | Val. Acc: 0.8875\n",
            "Epoch: 18\n",
            "\tTrain Loss: 0.8688 | Train Acc: 0.8688\n",
            "\t Val. Loss: 0.3645 | Val. Acc: 0.8875\n",
            "Epoch: 19\n",
            "\tTrain Loss: 0.8894 | Train Acc: 0.8894\n",
            "\t Val. Loss: 0.3819 | Val. Acc: 0.8725\n",
            "Epoch: 20\n",
            "\tTrain Loss: 0.8762 | Train Acc: 0.8762\n",
            "\t Val. Loss: 0.3707 | Val. Acc: 0.8825\n",
            "Epoch: 21\n",
            "\tTrain Loss: 0.9031 | Train Acc: 0.9031\n",
            "\t Val. Loss: 0.3884 | Val. Acc: 0.8875\n",
            "Epoch: 22\n",
            "\tTrain Loss: 0.8856 | Train Acc: 0.8856\n",
            "\t Val. Loss: 0.3591 | Val. Acc: 0.8950\n",
            "Epoch: 23\n",
            "\tTrain Loss: 0.8881 | Train Acc: 0.8881\n",
            "\t Val. Loss: 0.3587 | Val. Acc: 0.8925\n",
            "Epoch: 24\n",
            "\tTrain Loss: 0.9000 | Train Acc: 0.9000\n",
            "\t Val. Loss: 0.3425 | Val. Acc: 0.8950\n",
            "Epoch: 25\n",
            "\tTrain Loss: 0.9006 | Train Acc: 0.9006\n",
            "\t Val. Loss: 0.3540 | Val. Acc: 0.8875\n",
            "Epoch: 26\n",
            "\tTrain Loss: 0.9037 | Train Acc: 0.9037\n",
            "\t Val. Loss: 0.3808 | Val. Acc: 0.8875\n",
            "Epoch: 27\n",
            "\tTrain Loss: 0.9075 | Train Acc: 0.9075\n",
            "\t Val. Loss: 0.3701 | Val. Acc: 0.8950\n",
            "Epoch: 28\n",
            "\tTrain Loss: 0.8950 | Train Acc: 0.8950\n",
            "\t Val. Loss: 0.3720 | Val. Acc: 0.8900\n",
            "Epoch: 29\n",
            "\tTrain Loss: 0.9113 | Train Acc: 0.9113\n",
            "\t Val. Loss: 0.3711 | Val. Acc: 0.9050\n",
            "Epoch: 30\n",
            "\tTrain Loss: 0.8975 | Train Acc: 0.8975\n",
            "\t Val. Loss: 0.3454 | Val. Acc: 0.9125\n",
            "Epoch: 31\n",
            "\tTrain Loss: 0.9181 | Train Acc: 0.9181\n",
            "\t Val. Loss: 0.3530 | Val. Acc: 0.9050\n",
            "Epoch: 32\n",
            "\tTrain Loss: 0.9150 | Train Acc: 0.9150\n",
            "\t Val. Loss: 0.3724 | Val. Acc: 0.8975\n",
            "Epoch: 33\n",
            "\tTrain Loss: 0.9163 | Train Acc: 0.9163\n",
            "\t Val. Loss: 0.3637 | Val. Acc: 0.9050\n",
            "Epoch: 34\n",
            "\tTrain Loss: 0.9219 | Train Acc: 0.9219\n",
            "\t Val. Loss: 0.3641 | Val. Acc: 0.9025\n",
            "Epoch: 35\n",
            "\tTrain Loss: 0.9138 | Train Acc: 0.9138\n",
            "\t Val. Loss: 0.3518 | Val. Acc: 0.9100\n",
            "Epoch: 36\n",
            "\tTrain Loss: 0.9194 | Train Acc: 0.9194\n",
            "\t Val. Loss: 0.3577 | Val. Acc: 0.9100\n",
            "Epoch: 37\n",
            "\tTrain Loss: 0.9200 | Train Acc: 0.9200\n",
            "\t Val. Loss: 0.3412 | Val. Acc: 0.9100\n",
            "Epoch: 38\n",
            "\tTrain Loss: 0.9137 | Train Acc: 0.9137\n",
            "\t Val. Loss: 0.3368 | Val. Acc: 0.9050\n",
            "Epoch: 39\n",
            "\tTrain Loss: 0.9231 | Train Acc: 0.9231\n",
            "\t Val. Loss: 0.3618 | Val. Acc: 0.9050\n",
            "Epoch: 40\n",
            "\tTrain Loss: 0.9294 | Train Acc: 0.9294\n",
            "\t Val. Loss: 0.3666 | Val. Acc: 0.9050\n",
            "Epoch: 41\n",
            "\tTrain Loss: 0.9206 | Train Acc: 0.9206\n",
            "\t Val. Loss: 0.3413 | Val. Acc: 0.9150\n",
            "Epoch: 42\n",
            "\tTrain Loss: 0.9181 | Train Acc: 0.9181\n",
            "\t Val. Loss: 0.3455 | Val. Acc: 0.9200\n",
            "Epoch: 43\n",
            "\tTrain Loss: 0.9406 | Train Acc: 0.9406\n",
            "\t Val. Loss: 0.3581 | Val. Acc: 0.9125\n",
            "Epoch: 44\n",
            "\tTrain Loss: 0.9250 | Train Acc: 0.9250\n",
            "\t Val. Loss: 0.3797 | Val. Acc: 0.9125\n",
            "Epoch: 45\n",
            "\tTrain Loss: 0.9244 | Train Acc: 0.9244\n",
            "\t Val. Loss: 0.3562 | Val. Acc: 0.9150\n",
            "Epoch: 46\n",
            "\tTrain Loss: 0.9338 | Train Acc: 0.9338\n",
            "\t Val. Loss: 0.3721 | Val. Acc: 0.9175\n",
            "Epoch: 47\n",
            "\tTrain Loss: 0.9325 | Train Acc: 0.9325\n",
            "\t Val. Loss: 0.3732 | Val. Acc: 0.9200\n",
            "Epoch: 48\n",
            "\tTrain Loss: 0.9350 | Train Acc: 0.9350\n",
            "\t Val. Loss: 0.3756 | Val. Acc: 0.9175\n",
            "Epoch: 49\n",
            "\tTrain Loss: 0.9281 | Train Acc: 0.9281\n",
            "\t Val. Loss: 0.3946 | Val. Acc: 0.9150\n",
            "Epoch: 50\n",
            "\tTrain Loss: 0.9262 | Train Acc: 0.9262\n",
            "\t Val. Loss: 0.3746 | Val. Acc: 0.9125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb1HXSiDKUfL"
      },
      "source": [
        "We have achieved an accuracy of approx 92% on training set and accuracy of around 66% on validation set. We can improve this by adding more hidden layers and changing the number of neurons. \n",
        "Even with 6 hidden layers and reduced and increased learning rate I was able to achieve only 65% accuracy. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzUajVFKLke7"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    }
  ]
}